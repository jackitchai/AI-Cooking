{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pythainlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWv-Z-TFCCFF",
        "outputId": "c2037696-e91b-4eae-cee7-0a42256052d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pythainlp\n",
            "  Downloading pythainlp-5.0.4-py3-none-any.whl (17.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.9/17.9 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from pythainlp) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (2024.6.2)\n",
            "Installing collected packages: pythainlp\n",
            "Successfully installed pythainlp-5.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0SblKC3xqOg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "from pythainlp.corpus import thai_stopwords\n",
        "import string\n",
        "\n",
        "# Function to preprocess Thai text\n",
        "def preprocess_text(text):\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(text, engine='newmm')\n",
        "    # Remove punctuation and stopwords\n",
        "    tokens = [word for word in tokens if word not in string.punctuation and word not in thai_stopwords()]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Load dev.csv\n",
        "dev_data = pd.read_csv('dev.csv')\n",
        "\n",
        "# Preprocess text\n",
        "dev_data['processed_text'] = dev_data['text'].apply(preprocess_text)\n",
        "\n",
        "# Vectorize text using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X = vectorizer.fit_transform(dev_data['processed_text'])\n",
        "\n",
        "# Function to extract top keywords\n",
        "def extract_keywords(text, vectorizer, top_n=5):\n",
        "    # Preprocess the text\n",
        "    text = preprocess_text(text)\n",
        "    # Vectorize the text\n",
        "    text_vectorized = vectorizer.transform([text])\n",
        "    # Calculate similarity with each document in X\n",
        "    similarities = cosine_similarity(text_vectorized, X)\n",
        "    # Get indices of top N most similar documents\n",
        "    top_indices = similarities.argsort()[0][-top_n:][::-1]\n",
        "    # Get corresponding keywords\n",
        "    keywords = []\n",
        "    for idx in top_indices:\n",
        "        keywords.extend(dev_data.iloc[idx]['keywords'].split('|'))\n",
        "    # Return unique keywords\n",
        "    return '|'.join(list(dict.fromkeys(keywords))[:top_n])\n",
        "\n",
        "# Load test.csv\n",
        "test_data = pd.read_csv('test.csv')\n",
        "\n",
        "# Generate predictions for test data\n",
        "test_data['keywords'] = test_data['text'].apply(lambda x: extract_keywords(x, vectorizer))\n",
        "\n",
        "# Save output to output.csv\n",
        "test_data[['id', 'keywords']].to_csv('output.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "erTF-Hpq0VPO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}